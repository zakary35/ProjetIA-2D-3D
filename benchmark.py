import os
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from typing import List, Dict, Any

# Import du pipeline unitaire
from src.video_pipeline import VideoPipeline

class BenchmarkRunner:
    """
    Orchestrateur de tests pour comparer diff√©rents mod√®les et m√©thodes de stabilisation.
    G√©n√®re un rapport complet (CSV + Graphiques).
    """

    # Configurations √† tester
    MODELS: List[str] = ['dav2', 'vda']
    METHODS: List[str] = ['raw', 'median', 'ema', 'confidence']
    
    def __init__(self, input_video: str, output_dir: str = "benchmark_results", limit_frames: int = None, device: str = 'cuda'):
        """
        Initialise le banc de test.

        Args:
            input_video (str): Chemin de la vid√©o source.
            output_dir (str): Dossier o√π tout sera sauvegard√©.
            limit_frames (int, optional): Nombre de frames pour un test rapide.
            device (str): 'cuda' ou 'cpu'.
        """
        self.input_video = input_video
        self.output_dir = output_dir
        self.limit_frames = limit_frames
        self.device = device
        self.results_data: List[Dict[str, Any]] = []
        
        # Cr√©ation du dossier
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Configuration graphique pro
        sns.set_theme(style="whitegrid")

    def run_all(self):
        """Lance la boucle d'exp√©rimentation sur toutes les combinaisons."""
        print(f"üöÄ D√©marrage du Benchmark Complet sur {self.device.upper()}...")
        print(f"üìÇ R√©sultats dans : {self.output_dir}")

        for model in self.MODELS:
            for method in self.METHODS:
                
                # Cas particulier : VDA a d√©j√† une coh√©rence temporelle. 
                # On peut vouloir tester 'raw' seulement, mais pour la science, testons tout.
                experiment_name = f"{model.upper()}_{method}"
                print(f"\nüß™ Test en cours : {experiment_name}")
                
                # D√©finition du fichier de sortie vid√©o
                video_out = os.path.join(self.output_dir, f"{experiment_name}.mp4")
                
                # Instanciation et ex√©cution du pipeline
                # On capture les erreurs pour ne pas arr√™ter le benchmark si un mod√®le plante
                try:
                    pipeline = VideoPipeline(
                        input_path=self.input_video,
                        output_path=video_out,
                        model_type=model,
                        model_size='vitl', # On garde le meilleur mod√®le pour le comparatif
                        device=self.device,
                        stabilize_method=method,
                        limit_frames=self.limit_frames
                    )
                    
                    stats = pipeline.run()
                    
                    # Enregistrement des donn√©es brutes
                    entry = {
                        'Model': model.upper(),
                        'Method': method.capitalize(),
                        'Configuration': experiment_name,
                        'Warp Error (L1)': stats['warping_error_mean'],
                        'LPIPS (Perceptual)': stats['lpips_mean'],
                        'Edge Alignment': stats['edge_alignment_mean'],
                        'FPS': stats['fps_process'],
                        # On garde les donn√©es PSD brutes pour le graph global
                        'psd_freqs': stats['psd_data']['freqs'],
                        'psd_power': stats['psd_data']['power']
                    }
                    self.results_data.append(entry)
                    
                except Exception as e:
                    print(f"‚ùå Erreur sur {experiment_name} : {e}")
                    continue

    def generate_report(self):
        """G√©n√®re le fichier CSV et les graphiques comparatifs."""
        if not self.results_data:
            print("‚ö†Ô∏è Aucun r√©sultat √† analyser.")
            return

        # 1. Cr√©ation du DataFrame Pandas
        df = pd.DataFrame(self.results_data)
        csv_path = os.path.join(self.output_dir, "final_scores.csv")
        df.to_csv(csv_path, index=False)
        print(f"üìÑ Tableau des scores sauvegard√© : {csv_path}")
        
        # On retire les colonnes PSD pour l'affichage console
        print("\n=== R√âSUM√â DES SCORES ===")
        print(df.drop(columns=['psd_freqs', 'psd_power']))

        # 2. Graphiques en Barres (Scores)
        self._plot_metric_comparison(df, 'Warp Error (L1)', 'Stabilit√© G√©om√©trique (Plus bas = Mieux)')
        self._plot_metric_comparison(df, 'LPIPS (Perceptual)', 'Stabilit√© Perceptuelle (Plus bas = Mieux)')
        self._plot_metric_comparison(df, 'Edge Alignment', 'Respect des Bords (Plus haut = Mieux)')
        
        # 3. Graphique PSD Comparatif (Multi-Line)
        self._plot_combined_psd()

    def _plot_metric_comparison(self, df: pd.DataFrame, metric: str, title: str):
        """G√©n√®re un bar chart group√©."""
        plt.figure(figsize=(10, 6))
        
        # Bar chart : X=Method, Y=Metric, Hue=Model
        sns.barplot(data=df, x='Method', y=metric, hue='Model', palette="viridis")
        
        plt.title(title)
        plt.ylabel(metric)
        plt.xlabel("M√©thode de Post-Traitement")
        plt.legend(title='Mod√®le')
        
        filename = f"comparison_{metric.split()[0].lower()}.png"
        plt.savefig(os.path.join(self.output_dir, filename))
        plt.close()

    def _plot_combined_psd(self):
        """G√©n√®re un graphique spectral superposant toutes les courbes."""
        plt.figure(figsize=(12, 8))
        
        for entry in self.results_data:
            freqs = np.array(entry['psd_freqs'])
            power = np.array(entry['psd_power'])
            label = entry['Configuration']
            
            # Style de ligne : Pointill√© pour VDA, Plein pour DAV2 (pour distinguer)
            linestyle = '--' if 'VDA' in label else '-'
            
            plt.plot(freqs, power, label=label, linestyle=linestyle, alpha=0.8, linewidth=1.5)

        plt.title("Analyse Spectrale du Flickering (PSD Global)")
        plt.xlabel("Fr√©quence (Hz)")
        plt.ylabel("Puissance du Bruit (Log)")
        plt.yscale('log') # Indispensable pour voir les diff√©rences
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # L√©gende √† l'ext√©rieur
        plt.grid(True, which="both", ls="-", alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(os.path.join(self.output_dir, "comparison_psd_all.png"))
        plt.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Benchmark Automatique DAV2 vs VDA")
    parser.add_argument('--input', type=str, required=True, help="Vid√©o d'entr√©e")
    parser.add_argument('--limit', type=int, default=None, help="Limite de frames (ex: 100 pour test)")
    
    args = parser.parse_args()
    
    # Lancement
    bench = BenchmarkRunner(input_video=args.input, limit_frames=args.limit)
    bench.run_all()
    bench.generate_report()