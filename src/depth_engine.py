import os
import torch
import torch.nn as nn
import numpy as np
import cv2
from typing import Optional, Union, Dict, Any

# Import des mod√®les (Assurez-vous que les chemins d'import sont bons dans votre projet)
from src.vda.video_depth_anything.video_depth_stream import VideoDepthAnything
from src.dav2.dpt import DepthAnythingV2

class BaseDepthEngine:
    """
    Classe abstraite d√©finissant l'interface pour les moteurs d'estimation de profondeur.
    G√®re la d√©tection du mat√©riel (CUDA/MPS/CPU).
    """
    def __init__(self, device: str = None):
        """
        Initialise le gestionnaire de p√©riph√©rique.

        Args:
            device (str, optional): 'cuda', 'mps' ou 'cpu'. Si None, d√©tecte automatiquement.
        """
        # 1. D√©tection automatique du GPU
        if device is None:
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
            elif torch.backends.mps.is_available():
                self.device = torch.device('mps')
            else:
                self.device = torch.device('cpu')
        else:
            self.device = torch.device(device)

        print(f"üöÄ [DepthEngine] Initialisation sur : {str(self.device).upper()}")
        self.model = None

    def infer(self, frame: np.ndarray) -> np.ndarray:
        """
        M√©thode abstraite pour l'inf√©rence.
        
        Args:
            frame (np.ndarray): Image source [H, W, 3] (BGR).

        Returns:
            np.ndarray: Carte de profondeur [H, W].
        
        Raises:
            NotImplementedError: Si la classe fille n'impl√©mente pas cette m√©thode.
        """
        raise NotImplementedError
    

class VDAEngine(BaseDepthEngine):
    """
    Moteur pour 'Video Depth Anything' (VDA).
    Optimis√© pour la coh√©rence temporelle gr√¢ce √† une gestion d'√©tat (Memory/State).
    """
    
    # Configuration architecturale du mod√®le (Static)
    CONFIGS = {
        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
    }

    def __init__(self, 
                 model_size: str = 'vitl', 
                 device: str = None, 
                 input_size: int = 518, 
                 fp32: bool = False,
                 checkpoint_path: Optional[str] = None):
        """
        Initialise le mod√®le Video Depth Anything.

        Args:
            model_size (str): Taille du mod√®le ('vits', 'vitb', 'vitl').
            device (str): P√©riph√©rique d'ex√©cution.
            input_size (int): Taille de redimensionnement interne pour l'inf√©rence.
            fp32 (bool): Si True, utilise float32 (plus lent, plus pr√©cis). Sinon float16.
            checkpoint_path (str, optional): Chemin sp√©cifique vers le fichier .pth.
        """
        super().__init__(device)
        self.input_size = input_size
        self.fp32 = fp32
        
        if model_size not in self.CONFIGS:
            raise ValueError(f"Mod√®le inconnu : {model_size}. Choix : {list(self.CONFIGS.keys())}")

        # 1. Instanciation de l'architecture
        print(f"üèóÔ∏è [VDA] Construction du mod√®le {model_size}...")
        self.model = VideoDepthAnything(**self.CONFIGS[model_size])

        # 2. Gestion du chemin des poids
        if checkpoint_path is None:
            # Chemin par d√©faut
            checkpoint_path = f'checkpoints/vda/video_depth_anything_{model_size}.pth'
        
        # 3. Chargement s√©curis√©
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"‚ùå [VDA] ERREUR : Poids introuvables √† : {checkpoint_path}")
            
        try:
            # map_location='cpu' √©vite de saturer la VRAM pendant le chargement initial
            state_dict = torch.load(checkpoint_path, map_location='cpu')
            self.model.load_state_dict(state_dict)
            self.model.to(self.device).eval() # Mode freeze (pas de gradients)
            print(f"‚úÖ [VDA] Mod√®le charg√© et pr√™t.")
        except Exception as e:
            raise RuntimeError(f"‚ùå [VDA] Erreur lors du chargement des poids : {e}")
        
    def infer(self, frame: np.ndarray) -> np.ndarray:
        """
        Calcule la profondeur en tenant compte de l'historique vid√©o.

        Args:
            frame (np.ndarray): Image actuelle [H, W, 3] en format BGR (Standard OpenCV).

        Returns:
            np.ndarray: Carte de profondeur brute [H, W] (Float32).
                        Note: Ce n'est pas normalis√© [0,1], c'est une disparit√© relative.
        """
        # Conversion BGR -> RGB requise par VDA
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Inf√©rence avec gestion du cache temporel interne
        # Le param√®tre 'infer_video_depth_one' sugg√®re un traitement image par image avec m√©moire
        depth = self.model.infer_video_depth_one(
            frame_rgb, 
            input_size=self.input_size, 
            device=self.device, 
            fp32=self.fp32
        )
        
        return depth


class DAV2Engine(BaseDepthEngine):
    """
    Moteur pour 'Depth Anything V2' (DAV2).
    Traitement image par image sans coh√©rence temporelle (Single Image Depth Estimation).
    """
    
    # Configuration architecturale
    CONFIGS = {
        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
        'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
    }

    def __init__(self, 
                 model_size: str = 'vitl', 
                 device: str = None, 
                 input_size: int = 518,
                 checkpoint_path: Optional[str] = None):
        """
        Initialise le mod√®le Depth Anything V2.

        Args:
            model_size (str): Taille du mod√®le ('vits', 'vitb', 'vitl', 'vitg').
            device (str): P√©riph√©rique.
            input_size (int): Taille d'entr√©e (518 est le standard pour DAV2).
            checkpoint_path (str, optional): Chemin vers les poids .pth.
        """
        super().__init__(device)
        self.input_size = input_size

        if model_size not in self.CONFIGS:
            raise ValueError(f"Mod√®le inconnu : {model_size}. Choix : {list(self.CONFIGS.keys())}")

        # 1. Instanciation
        print(f"üèóÔ∏è [DAV2] Construction du mod√®le {model_size}...")
        self.model = DepthAnythingV2(**self.CONFIGS[model_size])

        # 2. Gestion du chemin
        if checkpoint_path is None:
            checkpoint_path = f'checkpoints/dav2/depth_anything_v2_{model_size}.pth'

        # 3. Chargement
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"‚ùå [DAV2] ERREUR : Poids introuvables √† : {checkpoint_path}")

        try:
            state_dict = torch.load(checkpoint_path, map_location='cpu')
            self.model.load_state_dict(state_dict)
            self.model.to(self.device).eval()
            print(f"‚úÖ [DAV2] Mod√®le charg√© et pr√™t.")
        except Exception as e:
            raise RuntimeError(f"‚ùå [DAV2] Erreur lors du chargement : {e}")

    def infer(self, frame: np.ndarray) -> np.ndarray:
        """
        Calcule la profondeur d'une image unique.

        Args:
            frame (np.ndarray): Image [H, W, 3] en BGR.

        Returns:
            np.ndarray: Carte de profondeur brute [H, W].
                        Attention : Les valeurs ne sont pas born√©es [0, 1].
        """
        # DepthAnythingV2.infer_image g√®re g√©n√©ralement la conversion BGR/RGB en interne 
        # ou attend du BGR standard OpenCV. Nous passons la frame brute.
        return self.model.infer_image(frame, input_size=self.input_size)